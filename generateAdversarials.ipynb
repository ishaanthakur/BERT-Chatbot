{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#\n",
    "# generateAdversarials: This code will load a pre-trained model, sample validation data\n",
    "#                       and find adversarial inputs.\n",
    "#\n",
    "# Method: Let the input to the forcaster be X and the target be y\n",
    "#         Note that both X & y are real valued\n",
    "#         We find X' such that X' ~ X and |y'-y| >> 0\n",
    "#         We will adapt the optimisation method of Carlini & Wagner\n",
    "#         for the generation of the adversarials\n",
    "#\n",
    "# Author: Anurag Dwarakanath\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATAFILE_VALIDATE = \"mock_kaggle_edit_validate.csv\"\n",
    "\n",
    "TRAINED_MODEL_PATH = 'savedModel'\n",
    "\n",
    "TIME_STEPS = 10 # i.e. look at the past 10 days and forecast\n",
    "NUMBER_OF_DAYS_TO_FORECAST = 1 # for now we will only forecast the next day's sales\n",
    "\n",
    "BATCH_SIZE=100\n",
    "\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the validation data\n",
    "rawData = pandas.read_csv(DATAFILE_VALIDATE)\n",
    "validationSales=rawData['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to normalise the data\n",
    "MIN = 0\n",
    "RANGE = 542\n",
    "validationSalesNormalised = [(i-MIN)/RANGE for i in validationSales]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the sequences\n",
    "validationSalesSequences = np.zeros(shape=(len(validationSales)-TIME_STEPS - NUMBER_OF_DAYS_TO_FORECAST + 1, TIME_STEPS, 1))\n",
    "validationSalesTargets = np.zeros(shape=(len(validationSales)-TIME_STEPS - NUMBER_OF_DAYS_TO_FORECAST + 1, NUMBER_OF_DAYS_TO_FORECAST))\n",
    "\n",
    "for i in range(len(validationSales)-TIME_STEPS - NUMBER_OF_DAYS_TO_FORECAST + 1):\n",
    "    validationSalesSequences[i,:,0] = validationSalesNormalised[i:i+TIME_STEPS]\n",
    "    validationSalesTargets[i,:] = validationSalesNormalised[i+TIME_STEPS:i+TIME_STEPS+NUMBER_OF_DAYS_TO_FORECAST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the data structure to hold the perturbations\n",
    "perturbedSequences = np.zeros(shape=(len(validationSales)-TIME_STEPS - NUMBER_OF_DAYS_TO_FORECAST + 1, TIME_STEPS, 1))\n",
    "perturbedForecasts = np.zeros(shape=(len(validationSales)-TIME_STEPS - NUMBER_OF_DAYS_TO_FORECAST + 1, NUMBER_OF_DAYS_TO_FORECAST))\n",
    "originalForecasts = np.zeros(shape=(len(validationSales)-TIME_STEPS - NUMBER_OF_DAYS_TO_FORECAST + 1, NUMBER_OF_DAYS_TO_FORECAST))\n",
    "\n",
    "inputSequenceLosses = np.zeros(shape=(len(validationSales)-TIME_STEPS - NUMBER_OF_DAYS_TO_FORECAST + 1, NUMBER_OF_DAYS_TO_FORECAST))\n",
    "forecastLosses = np.zeros(shape=(len(validationSales)-TIME_STEPS - NUMBER_OF_DAYS_TO_FORECAST + 1, NUMBER_OF_DAYS_TO_FORECAST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model from: savedModel\n",
      "INFO:tensorflow:Restoring parameters from savedModel\\variables\\variables\n",
      "2\n",
      "Starting Batch: 0\n",
      "Tensor(\"forecast_original_scale:0\", shape=(?, 1), dtype=float32)\n",
      "current 2 Forecast for sequence number  0 is:  [[57.43527 ]\n",
      " [46.846264]]\n",
      "Starting Batch: 1\n",
      "Tensor(\"forecast_original_scale:0\", shape=(?, 1), dtype=float32)\n",
      "current 2 Forecast for sequence number  1 is:  [[33.79866 ]\n",
      " [36.176975]]\n",
      "Mininum sequence loss: [1.19921629e-06]\n",
      "Perturbed sequence: [[ 77.97800446]\n",
      " [121.97356415]\n",
      " [164.01097107]\n",
      " [288.0302124 ]\n",
      " [138.03642273]\n",
      " [188.99427795]\n",
      " [320.95986938]\n",
      " [209.05773926]\n",
      " [161.01622009]\n",
      " [157.58609009]]\n",
      "Original Sequence: [[ 78.]\n",
      " [122.]\n",
      " [164.]\n",
      " [288.]\n",
      " [138.]\n",
      " [189.]\n",
      " [321.]\n",
      " [209.]\n",
      " [161.]\n",
      " [157.]]\n",
      "Original Forecast: [73.00727081]\n",
      "Perturbed Forecast: [146.01014709]\n",
      "Actual target: [283.]\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "#We now load the pre-trained graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    perturbVariables=tf.get_variable(name='pVar', shape=(BATCH_SIZE, TIME_STEPS, 1), dtype=tf.float32)\n",
    "    #perturbVariables=tf.Variable(name='pVar', initial_value=np.zeros(size=(None, TIME_STEPS, 1)), dtype=tf.float32, validate_shape=False)\n",
    "    perturbedSequence = tf.math.square(x=perturbVariables) # We want the perturbed Sequence to always be positive.\n",
    "\n",
    "    actualValidationInputSequence = tf.placeholder(name='aInp', shape=(None, TIME_STEPS, 1), dtype=tf.float32)\n",
    "    #actualValidationTarget = tf.placeholder(name='aTar', shape=(None, NUMBER_OF_DAYS_TO_FORECAST), dtype=tf.float32)\n",
    "    \n",
    "    sess.run(tf.variables_initializer([perturbVariables]))\n",
    "    \n",
    "    print('Loading the model from:', TRAINED_MODEL_PATH)\n",
    "    tf.saved_model.loader.load(sess=sess, export_dir=TRAINED_MODEL_PATH, tags=[tag_constants.SERVING], input_map={'inputSequencePlaceholder:0':perturbedSequence})\n",
    "    \n",
    "    #inputSequence = tf.get_default_graph().get_tensor_by_name('inputSequencePlaceholder:0')\n",
    "    forecast_normalisedScale = tf.get_default_graph().get_tensor_by_name('forecast_normalised_scale:0')\n",
    "    forecast_originalScale = tf.get_default_graph().get_tensor_by_name('forecast_original_scale:0')\n",
    "    targetForecast = tf.get_default_graph().get_tensor_by_name('targetPlaceholder:0')\n",
    "    \n",
    "    minLoss = 1000\n",
    "    minLossSequenceID=-1\n",
    "    #loop through all the validation sequences\n",
    "    start=0\n",
    "    end=0\n",
    "    numIterations = np.int(np.ceil(len(validationSalesTargets)/BATCH_SIZE))\n",
    "    print(numIterations)\n",
    "    currentSequence = np.zeros(shape=(BATCH_SIZE, TIME_STEPS, 1))\n",
    "    for i in range(numIterations):\n",
    "        \n",
    "        print('Starting Batch:', i)\n",
    "        start=i*BATCH_SIZE\n",
    "        if (start+BATCH_SIZE < len(validationSalesTargets)):\n",
    "            end=start+BATCH_SIZE\n",
    "        else:\n",
    "            end=len(validationSalesTargets)\n",
    "\n",
    "        #get the forecast for the current inputSeqeunce\n",
    "        currentSequence[0:end-start] = validationSalesSequences[start:end]\n",
    "            \n",
    "        #initialise perturbVariables with the actual values received.\n",
    "        assignValue = tf.assign(ref=perturbVariables, value=currentSequence)\n",
    "        sess.run(assignValue)\n",
    "    \n",
    "        #Get the forecasts for the actual values\n",
    "        tarFor = sess.run(forecast_originalScale)\n",
    "        print(forecast_originalScale)\n",
    "    \n",
    "        print('current 2 Forecast for sequence number ', i, 'is: ', tarFor[0:2])\n",
    "    \n",
    "        #random initialisation of perturbed Variables\n",
    "        #assignValue = tf.assign(ref=perturbVariables, value=currentSequence)# + np.random.normal(size=(1, TIME_STEPS, 1)))\n",
    "        #sess.run(assignValue)\n",
    "    \n",
    "        #inputSequenceLoss = tf.nn.l2_loss(t=(perturbedSequence - currentSequence))\n",
    "        inputSequenceLoss = tf.math.reduce_sum(input_tensor=tf.math.square(perturbedSequence - currentSequence), axis=1)\n",
    "        #inputSequenceLoss = tf.math.reduce_max(input_tensor=tf.math.abs(perturbVar - currentSequence))\n",
    "        #forecastLoss = 1/(tf.losses.mean_squared_error(labels=forecast_originalScale, predictions=tarFor) + 0.00001)\n",
    "        #forecastLoss = -tf.math.log(tf.math.abs(forecast_originalScale - tarFor)/tf.math.abs(forecast_originalScale))\n",
    "        forecastLoss =  tf.math.square(forecast_originalScale - 2*tarFor)\n",
    "    \n",
    "        totalLoss = 1000* inputSequenceLoss + 1 * forecastLoss\n",
    "    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "        trainStep = optimizer.minimize(totalLoss, var_list=[perturbVariables])\n",
    "    \n",
    "        all_variables = tf.all_variables()\n",
    "        is_not_initialized   = sess.run([tf.is_variable_initialized(var) for var in all_variables])\n",
    "        not_initialized_vars = [v for (v, f) in zip(all_variables, is_not_initialized) if not f]\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))    \n",
    "    \n",
    "        for optStep in range(200):\n",
    "            _, perturbedInput, forecastForPerturbedInput, inpSeqLoss, forLoss = sess.run([trainStep, perturbedSequence, forecast_originalScale, inputSequenceLoss, forecastLoss])\n",
    "    \n",
    "\n",
    "        #save the values\n",
    "        perturbedSequences[start:end] = (perturbedInput[0:end-start] * RANGE) + MIN\n",
    "        perturbedForecasts[start:end] = forecastForPerturbedInput[0:end-start]\n",
    "        inputSequenceLosses[start:end] = inpSeqLoss[0:end-start]\n",
    "        forecastLosses[start:end] = forLoss[0:end-start]\n",
    "        originalForecasts[start:end] = tarFor[0:end-start]\n",
    "        \n",
    "        if np.min(inpSeqLoss) < minLoss :\n",
    "            minLoss = np.min(inpSeqLoss)\n",
    "            minLossSequenceID = start+np.argmin(inpSeqLoss)\n",
    "        \n",
    "        start+=BATCH_SIZE\n",
    "    \n",
    "    print('Mininum sequence loss:', inputSequenceLosses[minLossSequenceID])\n",
    "    print('Perturbed sequence:',perturbedSequences[minLossSequenceID])\n",
    "    print('Original Sequence:', (validationSalesSequences[minLossSequenceID] * RANGE ) + MIN )\n",
    "    print('Original Forecast:', originalForecasts[minLossSequenceID])\n",
    "    print('Perturbed Forecast:', perturbedForecasts[minLossSequenceID])\n",
    "    print('Actual target:',(validationSalesTargets[minLossSequenceID]*RANGE) + MIN )\n",
    "    print('--------')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the maximum amount of perturbation needed to double the forecast.\n",
    "# m will hold the maximum change needed in any time dimension for each of the input sequence\n",
    "m = np.max(np.abs(perturbedSequences - (validationSalesSequences * RANGE ) + MIN), axis=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10.,  7.,  6.,  3., 32., 53., 24.]),\n",
       " array([ 1,  2,  3,  4,  5, 10, 20, 30]),\n",
       " <a list of 7 Patch objects>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We now plot the histogram\n",
    "\n",
    "plt.hist(m, bins=[1, 2, 3, 4, 5, 10, 20, 30], rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's all folks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
